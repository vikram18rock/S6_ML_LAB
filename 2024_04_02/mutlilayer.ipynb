{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 209)\n",
      "(1, 209)\n",
      "(12288, 50)\n",
      "(1, 50)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.loadtxt('cat_train_x.csv', delimiter = ',')/255.0\n",
    "Y_train = np.loadtxt('cat_train_y.csv', delimiter = ',').reshape(1, X_train.shape[1])\n",
    "X_test = np.loadtxt('cat_test_x.csv', delimiter = ',')/255.0\n",
    "Y_test = np.loadtxt('cat_test_y.csv', delimiter = ',').reshape(1, X_test.shape[1])\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "def softmax(z):\n",
    "    expZ = np.exp(z)\n",
    "    return expZ/(np.sum(expZ, 0))\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    return A\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def derivative_relu(Z):\n",
    "    return np.array(Z > 0, dtype = 'float')\n",
    "\n",
    "def derivative_tanh(x):\n",
    "    return (1 - np.power(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(a,y):\n",
    "    n, m = y.shape\n",
    "    cost = 0\n",
    "    if n==1 :\n",
    "        cost = 1/m * (np.dot(y,np.log(a).T) + np.dot(1-y,np.log(1-a).T))\n",
    "    else :\n",
    "        cost = 1/m*np.sum(y*np.log(a))\n",
    "    cost = np.squeeze(cost)\n",
    "    return -1*cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layers):\n",
    "    parameters = {}\n",
    "    l = len(layers)\n",
    "    for i in range(1,l):\n",
    "        parameters[\"w\"+str(i)] = np.random.randn(layers[i],layers[i-1])/np.sqrt(layers[i-1])\n",
    "        parameters[\"b\"+str(i)] = np.zeros((layers[i],1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x,parameters,activation_function):\n",
    "    l = len(parameters)//2\n",
    "    forward_cache = {}\n",
    "    forward_cache[\"a0\"] = x\n",
    "    for i in range(1,l):\n",
    "        forward_cache[\"z\"+str(i)] = parameters[\"w\"+str(i)].dot(forward_cache[\"a\"+str(i-1)]) + parameters[\"b\"+str(i)]\n",
    "        if  activation_function == 'tanh':\n",
    "            forward_cache[\"a\"+str(i)] = tanh(forward_cache[\"z\"+str(i)])\n",
    "        else:\n",
    "            forward_cache[\"a\"+str(i)] = relu(forward_cache[\"z\"+str(i)])\n",
    "\n",
    "    forward_cache[\"z\"+str(l)] = parameters[\"w\"+str(l)].dot(forward_cache[\"a\"+str(l-1)]) + parameters[\"b\"+str(l)]\n",
    "\n",
    "    if forward_cache['z' + str(l)].shape[0] == 1:\n",
    "        forward_cache['a' + str(l)] = sigmoid(forward_cache['z' + str(l)])\n",
    "    else :\n",
    "        forward_cache['a' + str(l)] = softmax(forward_cache['z' + str(l)])\n",
    "\n",
    "    return forward_cache['a' + str(l)],forward_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of W1: (100, 12288)\n",
      "Shape of B1: (100, 1) \n",
      "\n",
      "Shape of W2: (200, 100)\n",
      "Shape of B2: (200, 1) \n",
      "\n",
      "Shape of W3: (1, 200)\n",
      "Shape of B3: (1, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "layers = [X_train.shape[0],100,200,Y_train.shape[0]]\n",
    "\n",
    "parameters = initialize_parameters(layers)\n",
    "\n",
    "for l in range(1, len(layers)):\n",
    "    print(\"Shape of W\" + str(l) + \":\", parameters['w' + str(l)].shape)\n",
    "    print(\"Shape of B\" + str(l) + \":\", parameters['b' + str(l)].shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A0 : (12288, 209)\n",
      "Shape of A1 : (100, 209)\n",
      "Shape of A2 : (200, 209)\n",
      "Shape of A3 : (1, 209)\n"
     ]
    }
   ],
   "source": [
    "al , forward_cache = forward_propagation(X_train, parameters, 'relu')\n",
    "\n",
    "for l in range(len(parameters)//2 + 1):\n",
    "    print(\"Shape of A\" + str(l) + \" :\", forward_cache['a' + str(l)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(al,y,parameters,forward_cache,activation):\n",
    "    grads = {}\n",
    "    l = len(parameters)//2\n",
    "    m = al.shape[1]\n",
    "    grads[\"dz\"+str(l)] = al - y\n",
    "    grads['dw'+str(l)] = (1/m)*np.dot(grads['dz'+str(l)],forward_cache['a'+str(l-1)].T)\n",
    "    grads['db'+str(l)] = (1/m)*np.sum(grads['dz'+str(l)],axis=1,keepdims=True)\n",
    "\n",
    "    for i in range(l-1,0,-1):\n",
    "        if activation == 'relu':\n",
    "            grads[\"dz\"+str(i)] = np.dot(parameters['w'+str(i+1)].T,grads['dz'+str(i+1)])*derivative_relu(forward_cache['a'+str(i)])\n",
    "        else:\n",
    "            grads[\"dz\"+str(i)] = np.dot(parameters['w'+str(i+1)].T,grads['dz'+str(i+1)])*derivative_tanh(forward_cache['a'+str(i)])\n",
    "        grads['dw'+str(i)] = (1/m)*np.dot(grads['dz'+str(i)],forward_cache['a'+str(i-1)].T)\n",
    "        grads['db'+str(i)] = (1/m)*np.sum(grads['dz'+str(i)],axis=1,keepdims=True)\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dz3 : (1, 209)\n",
      "Shape of dw3 : (1, 200)\n",
      "Shape of db3 : (1, 1) \n",
      "\n",
      "Shape of dz2 : (200, 209)\n",
      "Shape of dw2 : (200, 100)\n",
      "Shape of db2 : (200, 1) \n",
      "\n",
      "Shape of dz1 : (100, 209)\n",
      "Shape of dw1 : (100, 12288)\n",
      "Shape of db1 : (100, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "grads = backward_propagation(al,Y_train, parameters, forward_cache, 'relu')\n",
    "\n",
    "for l in reversed(range(1, len(grads)//(len(layers)-1) + 1)):\n",
    "    print(\"Shape of dz\" + str(l) + \" :\", grads['dz' + str(l)].shape)\n",
    "    print(\"Shape of dw\" + str(l) + \" :\", grads['dw' + str(l)].shape)\n",
    "    print(\"Shape of db\" + str(l) + \" :\", grads['db' + str(l)].shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,grads,learning_rate):\n",
    "    l = len(parameters) // 2\n",
    "    for i in range(l):\n",
    "        parameters[\"w\" + str(i+1)] = parameters[\"w\" + str(i+1)] - learning_rate * grads[\"dw\" + str(i+1)]\n",
    "        parameters[\"b\" + str(i+1)] = parameters[\"b\" + str(i+1)] - learning_rate * grads[\"db\" + str(i+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters, activation):\n",
    "\n",
    "    m = X.shape[1]\n",
    "    y_pred, caches = forward_propagation(X, parameters, activation)\n",
    "    \n",
    "    if y.shape[0] == 1:\n",
    "        y_pred = np.array(y_pred > 0.5, dtype = 'float')\n",
    "    else:\n",
    "        y = np.argmax(y, 0)\n",
    "        y_pred = np.argmax(y_pred, 0)\n",
    "    \n",
    "    return np.round(np.sum((y_pred == y)/m), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate = 0.03, activation = 'relu', num_iterations = 3000):#lr was 0.009\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []              \n",
    "    \n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        AL, forward_cache = forward_propagation(X, parameters, activation)\n",
    "\n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "        grads = backward_propagation(AL, Y, parameters, forward_cache, activation)\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if i % (num_iterations/10) == 0:\n",
    "            print(\"\\niter:{} \\t cost: {} \\t train_acc:{} \\t test_acc:{}\".format(i, np.round(cost, 2), predict(X_train, Y_train, parameters, activation), predict(X_test, Y_test, parameters, activation)))\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(\"==\", end = '')\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iter:0 \t cost: 0.77 \t train_acc:0.51 \t test_acc:0.42\n",
      "===="
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "iter:250 \t cost: 0.63 \t train_acc:0.74 \t test_acc:0.64\n",
      "==================================================\n",
      "iter:500 \t cost: 0.54 \t train_acc:0.78 \t test_acc:0.7\n",
      "==================================================\n",
      "iter:750 \t cost: 0.44 \t train_acc:0.92 \t test_acc:0.78\n",
      "==================================================\n",
      "iter:1000 \t cost: 0.32 \t train_acc:0.96 \t test_acc:0.8\n",
      "==================================================\n",
      "iter:1250 \t cost: 0.23 \t train_acc:0.98 \t test_acc:0.76\n",
      "==================================================\n",
      "iter:1500 \t cost: 0.16 \t train_acc:0.98 \t test_acc:0.82\n",
      "==================================================\n",
      "iter:1750 \t cost: 0.13 \t train_acc:0.98 \t test_acc:0.8\n",
      "==================================================\n",
      "iter:2000 \t cost: 0.11 \t train_acc:0.98 \t test_acc:0.8\n",
      "==================================================\n",
      "iter:2250 \t cost: 0.1 \t train_acc:0.98 \t test_acc:0.8\n",
      "=================================================="
     ]
    }
   ],
   "source": [
    "layers_dims = [X_train.shape[0], 20, 7, 5, Y_train.shape[0]] #  4-layer model\n",
    "lr = 0.0075\n",
    "iters = 2500\n",
    "\n",
    "parameters = model(X_train, Y_train, layers_dims, learning_rate = lr, activation = 'relu', num_iterations = iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
